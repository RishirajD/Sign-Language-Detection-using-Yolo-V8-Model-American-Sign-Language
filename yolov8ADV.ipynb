{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "# Check if libraries are properly imported\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path found: D:\\objdetection_imp_model\\Image\n",
      "Sample directories: ['A', 'B', 'C', 'D', 'E']\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Define dataset path and check if it exists\n",
    "dataset_path = r\"D:\\objdetection_imp_model\\Image\"\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Dataset path found: {dataset_path}\")\n",
    "    # List a few sample directories to verify\n",
    "    sample_dirs = os.listdir(dataset_path)[:5]  # First 5 directories\n",
    "    print(f\"Sample directories: {sample_dirs}\")\n",
    "else:\n",
    "    print(f\"Warning: Dataset path not found: {dataset_path}\")\n",
    "    # You might need to update the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Define the HandGestureRecognition class\n",
    "class HandGestureRecognition:\n",
    "    def __init__(self, dataset_path, output_path='runs/detect'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.output_path = output_path\n",
    "        self.model = None\n",
    "        self.class_names = [chr(i + ord('A')) for i in range(26)]  # A to Z\n",
    "        self.roi_x, self.roi_y = 100, 100\n",
    "        self.roi_width, self.roi_height = 300, 300\n",
    "        print(f\"HandGestureRecognition initialized with classes: {self.class_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandGestureRecognition initialized with classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "HandGestureRecognition instance created\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Create an instance of the class\n",
    "hgr = HandGestureRecognition(dataset_path)\n",
    "print(\"HandGestureRecognition instance created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Add the prepare_dataset method to the class\n",
    "def prepare_dataset(self, yaml_file=\"hand_gestures.yaml\"):\n",
    "    \"\"\"\n",
    "    Prepares dataset in YOLOv8 format.\n",
    "    YOLOv8 requires images and labels in specific format.\n",
    "    \"\"\"\n",
    "    # Create directory structure\n",
    "    dataset_dir = \"hand_gestures_dataset\"\n",
    "    if os.path.exists(dataset_dir):\n",
    "        shutil.rmtree(dataset_dir)\n",
    "    \n",
    "    os.makedirs(f\"{dataset_dir}/train/images\", exist_ok=True)\n",
    "    os.makedirs(f\"{dataset_dir}/train/labels\", exist_ok=True)\n",
    "    os.makedirs(f\"{dataset_dir}/val/images\", exist_ok=True)\n",
    "    os.makedirs(f\"{dataset_dir}/val/labels\", exist_ok=True)\n",
    "    \n",
    "    # Create YAML file for dataset\n",
    "    with open(yaml_file, \"w\") as f:\n",
    "        f.write(f\"path: {os.path.abspath(dataset_dir)}\\n\")\n",
    "        f.write(\"train: train/images\\n\")\n",
    "        f.write(\"val: val/images\\n\")\n",
    "        f.write(\"nc: 26\\n\")  # Number of classes (A-Z)\n",
    "        f.write(f\"names: {self.class_names}\\n\")\n",
    "    \n",
    "    print(\"Converting dataset to YOLOv8 format...\")\n",
    "    \n",
    "    # Process each alphabet folder\n",
    "    processed_images = 0\n",
    "    for idx, alphabet in enumerate(self.class_names):\n",
    "        folder_path = os.path.join(self.dataset_path, alphabet)\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Warning: Folder for {alphabet} not found at {folder_path}\")\n",
    "            continue\n",
    "            \n",
    "        images = [f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        print(f\"Processing {alphabet}: {len(images)} images found\")\n",
    "        \n",
    "        # Split into train (80%) and validation (20%) sets\n",
    "        split_idx = int(len(images) * 0.8)\n",
    "        train_images = images[:split_idx]\n",
    "        val_images = images[split_idx:]\n",
    "        \n",
    "        # Process training images\n",
    "        for img_file in train_images:\n",
    "            img_path = os.path.join(folder_path, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "                \n",
    "            # Create label file (YOLOv8 format: class_id center_x center_y width height)\n",
    "            dest_img_path = f\"{dataset_dir}/train/images/{alphabet}_{img_file}\"\n",
    "            dest_label_path = f\"{dataset_dir}/train/labels/{alphabet}_{os.path.splitext(img_file)[0]}.txt\"\n",
    "            \n",
    "            # Copy image\n",
    "            shutil.copy(img_path, dest_img_path)\n",
    "            \n",
    "            # Create label (assuming object is centered and takes up ~70% of image)\n",
    "            with open(dest_label_path, \"w\") as f:\n",
    "                f.write(f\"{idx} 0.5 0.5 0.7 0.7\\n\")\n",
    "            \n",
    "            processed_images += 1\n",
    "        \n",
    "        # Process validation images (similar to training)\n",
    "        for img_file in val_images:\n",
    "            img_path = os.path.join(folder_path, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "                \n",
    "            dest_img_path = f\"{dataset_dir}/val/images/{alphabet}_{img_file}\"\n",
    "            dest_label_path = f\"{dataset_dir}/val/labels/{alphabet}_{os.path.splitext(img_file)[0]}.txt\"\n",
    "            \n",
    "            # Copy image\n",
    "            shutil.copy(img_path, dest_img_path)\n",
    "            \n",
    "            # Create label\n",
    "            with open(dest_label_path, \"w\") as f:\n",
    "                f.write(f\"{idx} 0.5 0.5 0.7 0.7\\n\")\n",
    "            \n",
    "            processed_images += 1\n",
    "    \n",
    "    print(f\"Dataset prepared successfully. Total processed images: {processed_images}\")\n",
    "    print(f\"YAML file created at {yaml_file}\")\n",
    "    return yaml_file\n",
    "\n",
    "# Add the method to the class\n",
    "HandGestureRecognition.prepare_dataset = prepare_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting dataset to YOLOv8 format...\n",
      "Processing A: 101 images found\n",
      "Processing B: 101 images found\n",
      "Processing C: 101 images found\n",
      "Processing D: 101 images found\n",
      "Processing E: 101 images found\n",
      "Processing F: 101 images found\n",
      "Processing G: 101 images found\n",
      "Processing H: 101 images found\n",
      "Processing I: 101 images found\n",
      "Processing J: 101 images found\n",
      "Processing K: 101 images found\n",
      "Processing L: 101 images found\n",
      "Processing M: 101 images found\n",
      "Processing N: 101 images found\n",
      "Processing O: 101 images found\n",
      "Processing P: 101 images found\n",
      "Processing Q: 101 images found\n",
      "Processing R: 101 images found\n",
      "Processing S: 101 images found\n",
      "Processing T: 101 images found\n",
      "Processing U: 101 images found\n",
      "Processing V: 101 images found\n",
      "Processing W: 101 images found\n",
      "Processing X: 101 images found\n",
      "Processing Y: 101 images found\n",
      "Processing Z: 101 images found\n",
      "Dataset prepared successfully. Total processed images: 2626\n",
      "YAML file created at hand_gestures.yaml\n",
      "Dataset preparation complete. YAML file: hand_gestures.yaml\n"
     ]
    }
   ],
   "source": [
    "# Block 6: Prepare the dataset\n",
    "yaml_file = hgr.prepare_dataset()\n",
    "print(f\"Dataset preparation complete. YAML file: {yaml_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: Add the train_model method to the class\n",
    "def train_model(self, yaml_file, epochs=35, img_size=640, batch_size=16):\n",
    "    \"\"\"Train YOLOv8 model on the prepared dataset\"\"\"\n",
    "    print(f\"Starting model training with {epochs} epochs...\")\n",
    "    \n",
    "    # Initialize YOLOv8 model\n",
    "    model = YOLO('yolov8n.pt')  # Start with pretrained nano model\n",
    "    \n",
    "    # Train the model\n",
    "    results = model.train(\n",
    "        data=yaml_file,\n",
    "        epochs=epochs,\n",
    "        imgsz=img_size,\n",
    "        batch=batch_size,\n",
    "        patience=10,  # Early stopping\n",
    "        name='hand_gesture_model'\n",
    "    )\n",
    "    \n",
    "    print(f\"Training completed. Model saved at {self.output_path}/hand_gesture_model\")\n",
    "    \n",
    "    # Load the best model\n",
    "    self.model = YOLO(f'{self.output_path}/hand_gesture_model/weights/best.pt')\n",
    "    return self.model\n",
    "\n",
    "# Add the method to the class\n",
    "HandGestureRecognition.train_model = train_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training with 35 epochs...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.25M/6.25M [00:01<00:00, 5.92MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.91  Python-3.10.11 torch-2.6.0+cpu CPU (AMD Ryzen 7 5700U with Radeon Graphics)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=hand_gestures.yaml, epochs=35, time=None, patience=10, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=hand_gesture_model, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\hand_gesture_model\n",
      "Overriding model.yaml nc=80 with nc=26\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    756382  ultralytics.nn.modules.head.Detect           [26, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,015,918 parameters, 3,015,902 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\hand_gesture_model', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\objdetection_imp_model\\hand_gestures_dataset\\train\\labels... 2080 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2080/2080 [00:03<00:00, 534.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\objdetection_imp_model\\hand_gestures_dataset\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\objdetection_imp_model\\hand_gestures_dataset\\val\\labels... 546 images, 0 backgrounds, 0 corrupt: 100%|██████████| 546/546 [00:01<00:00, 497.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\objdetection_imp_model\\hand_gestures_dataset\\val\\labels.cache\n",
      "Plotting labels to runs\\detect\\hand_gesture_model\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000333, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\hand_gesture_model\u001b[0m\n",
      "Starting training for 35 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/35         0G      1.329      4.171      1.834         40        640: 100%|██████████| 130/130 [09:11<00:00,  4.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:51<00:00,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.323      0.417      0.199      0.163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/35         0G      0.665      2.972        1.2         37        640: 100%|██████████| 130/130 [09:30<00:00,  4.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:51<00:00,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.495      0.739       0.67      0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/35         0G     0.5607      2.211      1.118         42        640: 100%|██████████| 130/130 [09:29<00:00,  4.38s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:48<00:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.685      0.835      0.854      0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/35         0G     0.4998      1.756       1.07         40        640: 100%|██████████| 130/130 [09:31<00:00,  4.40s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:47<00:00,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.859      0.957      0.979      0.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/35         0G     0.4455       1.45      1.028         47        640: 100%|██████████| 130/130 [09:19<00:00,  4.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:48<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.939      0.926      0.986      0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/35         0G     0.4007      1.253      1.002         40        640: 100%|██████████| 130/130 [09:25<00:00,  4.35s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:49<00:00,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.925      0.957      0.988      0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/35         0G     0.3814      1.122     0.9877         36        640: 100%|██████████| 130/130 [09:21<00:00,  4.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:48<00:00,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.965      0.988      0.991      0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/35         0G     0.3737      1.046     0.9857         45        640: 100%|██████████| 130/130 [09:21<00:00,  4.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:48<00:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.971      0.984      0.994      0.981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/35         0G     0.3413     0.9848     0.9693         37        640: 100%|██████████| 130/130 [09:20<00:00,  4.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:49<00:00,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.979      0.992      0.995      0.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/35         0G     0.3269      0.901     0.9617         42        640: 100%|██████████| 130/130 [09:27<00:00,  4.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:49<00:00,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.989      0.988      0.992      0.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/35         0G     0.3144     0.8536     0.9572         49        640: 100%|██████████| 130/130 [09:25<00:00,  4.35s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:51<00:00,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.983      0.993      0.995      0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/35         0G     0.2949     0.8082     0.9411         44        640: 100%|██████████| 130/130 [09:21<00:00,  4.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:50<00:00,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.952      0.979      0.995      0.963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/35         0G     0.2938     0.7647     0.9387         53        640: 100%|██████████| 130/130 [09:27<00:00,  4.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:51<00:00,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.978      0.992      0.995      0.983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/35         0G      0.278     0.7481     0.9323         38        640: 100%|██████████| 130/130 [09:22<00:00,  4.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:53<00:00,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.989      0.991      0.995      0.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/35         0G     0.2811     0.7224     0.9392         39        640: 100%|██████████| 130/130 [09:26<00:00,  4.35s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:53<00:00,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.982      0.995      0.995      0.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/35         0G     0.2651     0.7185     0.9264         38        640: 100%|██████████| 130/130 [09:23<00:00,  4.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:52<00:00,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.958      0.996      0.994      0.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/35         0G     0.2571     0.6828     0.9208         40        640: 100%|██████████| 130/130 [09:16<00:00,  4.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:52<00:00,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.985      0.991      0.995      0.989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/35         0G     0.2512     0.6355      0.917         32        640: 100%|██████████| 130/130 [09:23<00:00,  4.34s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:52<00:00,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.985      0.994      0.995      0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/35         0G      0.241      0.612     0.9217         42        640: 100%|██████████| 130/130 [09:15<00:00,  4.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:53<00:00,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.986      0.999      0.995      0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/35         0G     0.2352     0.6029     0.9152         42        640: 100%|██████████| 130/130 [09:16<00:00,  4.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:53<00:00,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.991      0.996      0.995      0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/35         0G     0.2234     0.5918     0.9069         40        640: 100%|██████████| 130/130 [09:13<00:00,  4.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:53<00:00,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.992      0.993      0.995      0.992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/35         0G     0.2267     0.5696     0.9084         44        640: 100%|██████████| 130/130 [09:20<00:00,  4.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:54<00:00,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.972      0.994      0.995      0.992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/35         0G     0.2118     0.5345     0.8975         45        640: 100%|██████████| 130/130 [09:41<00:00,  4.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [01:06<00:00,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.994      0.993      0.995      0.992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/35         0G     0.2085     0.5261     0.8995         45        640: 100%|██████████| 130/130 [11:00<00:00,  5.08s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:56<00:00,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.988      0.986      0.995      0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/35         0G     0.2086     0.5263     0.8993         41        640: 100%|██████████| 130/130 [10:36<00:00,  4.90s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:54<00:00,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.993      0.993      0.995      0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/35         0G     0.2422     0.4757     0.9161         16        640: 100%|██████████| 130/130 [09:20<00:00,  4.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:52<00:00,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.981      0.998      0.995      0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/35         0G     0.2059     0.4155     0.8863         16        640: 100%|██████████| 130/130 [09:16<00:00,  4.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:51<00:00,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.989      0.995      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/35         0G     0.1925     0.3918     0.8751         16        640: 100%|██████████| 130/130 [09:22<00:00,  4.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:53<00:00,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.994      0.997      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/35         0G     0.1873     0.3768     0.8791         16        640: 100%|██████████| 130/130 [09:20<00:00,  4.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:52<00:00,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.993      0.996      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/35         0G     0.1777     0.3617     0.8629         16        640: 100%|██████████| 130/130 [09:15<00:00,  4.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:53<00:00,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.994      0.997      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/35         0G     0.1712      0.353     0.8531         16        640: 100%|██████████| 130/130 [09:17<00:00,  4.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:51<00:00,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.994      0.996      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/35         0G     0.1586     0.3384     0.8423         16        640: 100%|██████████| 130/130 [09:22<00:00,  4.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:51<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.993      0.998      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/35         0G     0.1534     0.3314     0.8451         16        640: 100%|██████████| 130/130 [09:30<00:00,  4.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:55<00:00,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.995      0.998      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/35         0G     0.1484      0.323     0.8323         16        640: 100%|██████████| 130/130 [09:23<00:00,  4.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:52<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.995      0.998      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/35         0G     0.1419     0.3162     0.8321         16        640: 100%|██████████| 130/130 [09:23<00:00,  4.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:48<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.993      0.999      0.995      0.995\n",
      "\n",
      "35 epochs completed in 6.029 hours.\n",
      "Optimizer stripped from runs\\detect\\hand_gesture_model\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\hand_gesture_model\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\hand_gesture_model\\weights\\best.pt...\n",
      "Ultralytics 8.3.91  Python-3.10.11 torch-2.6.0+cpu CPU (AMD Ryzen 7 5700U with Radeon Graphics)\n",
      "Model summary (fused): 72 layers, 3,010,718 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:37<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        546        546      0.993      0.999      0.995      0.995\n",
      "                     A         21         21          1      0.965      0.995      0.995\n",
      "                     B         21         21      0.993          1      0.995      0.995\n",
      "                     C         21         21      0.993          1      0.995      0.995\n",
      "                     D         21         21      0.993          1      0.995      0.995\n",
      "                     E         21         21      0.992          1      0.995      0.995\n",
      "                     F         21         21      0.993          1      0.995      0.995\n",
      "                     G         21         21      0.998          1      0.995      0.995\n",
      "                     H         21         21      0.993          1      0.995      0.995\n",
      "                     I         21         21      0.994          1      0.995      0.995\n",
      "                     J         21         21      0.993          1      0.995      0.995\n",
      "                     K         21         21      0.993          1      0.995      0.995\n",
      "                     L         21         21      0.993          1      0.995      0.995\n",
      "                     M         21         21      0.989          1      0.995      0.995\n",
      "                     N         21         21      0.993          1      0.995      0.995\n",
      "                     O         21         21      0.993          1      0.995      0.995\n",
      "                     P         21         21      0.993          1      0.995      0.995\n",
      "                     Q         21         21      0.992          1      0.995      0.995\n",
      "                     R         21         21      0.993          1      0.995      0.995\n",
      "                     S         21         21      0.992          1      0.995      0.995\n",
      "                     T         21         21      0.993          1      0.995      0.995\n",
      "                     U         21         21      0.989          1      0.995      0.995\n",
      "                     V         21         21      0.994          1      0.995      0.995\n",
      "                     W         21         21      0.993          1      0.995      0.995\n",
      "                     X         21         21      0.992          1      0.995      0.995\n",
      "                     Y         21         21      0.992          1      0.995      0.995\n",
      "                     Z         21         21      0.993          1      0.995      0.995\n",
      "Speed: 1.5ms preprocess, 59.2ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\hand_gesture_model\u001b[0m\n",
      "Training completed. Model saved at runs/detect/hand_gesture_model\n",
      "Model training complete\n"
     ]
    }
   ],
   "source": [
    "# Block 8: Train the model (you can adjust epochs for faster testing)\n",
    "# Uncomment this block when you're ready to train\n",
    "model = hgr.train_model(yaml_file, epochs=35)  # Reduce epochs for testing\n",
    "print(\"Model training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file found at: runs/detect/hand_gesture_model/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if model file exists\n",
    "model_path = 'runs/detect/hand_gesture_model/weights/best.pt'\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Model file found at: {model_path}\")\n",
    "else:\n",
    "    print(f\"Model file not found at: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model type: <class 'ultralytics.models.yolo.model.YOLO'>\n",
      "Model architecture: {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'}\n"
     ]
    }
   ],
   "source": [
    "# Try to load the model\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    model = YOLO('runs/detect/hand_gesture_model/weights/best.pt')\n",
    "    print(\"Model loaded successfully!\")\n",
    "    # You can print some model information\n",
    "    print(f\"Model type: {type(model)}\")\n",
    "    print(f\"Model architecture: {model.names}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit. Press 'Enter' to add the word to the sentence. Press 'Space' to add a space.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class HandGestureRecognition:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = YOLO(model_path)\n",
    "        self.class_names = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'}\n",
    "    \n",
    "    def real_time_detection(self):\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        cap.set(3, 1280)  # Set width to 1280\n",
    "        cap.set(4, 720)   # Set height to 720\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open webcam.\")\n",
    "            return\n",
    "        \n",
    "        print(\"Press 'q' to quit. Press 'Enter' to add the word to the sentence. Press 'Space' to add a space.\")\n",
    "        \n",
    "        prev_time = 0\n",
    "        gesture_history = []\n",
    "        current_word = []  \n",
    "        sentence = \"\"\n",
    "        last_prediction = None  \n",
    "        confidence_threshold = 0.65\n",
    "        debounce_time = 0.5\n",
    "        last_prediction_time = time.time()\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            \n",
    "            frame = cv2.flip(frame, 1)\n",
    "            screen_height, screen_width = frame.shape[:2]\n",
    "\n",
    "            # ROI settings\n",
    "            roi_width, roi_height = 300, 300\n",
    "            roi_start_x, roi_start_y = screen_width - roi_width - 50, (screen_height - roi_height) // 2\n",
    "            roi_end_x, roi_end_y = roi_start_x + roi_width, roi_start_y + roi_height\n",
    "            \n",
    "            roi = frame[roi_start_y:roi_end_y, roi_start_x:roi_end_x]\n",
    "            results = self.model(roi, verbose=False)\n",
    "            \n",
    "            predicted_label, predicted_confidence = None, 0\n",
    "            if results and len(results) > 0:\n",
    "                for result in results:\n",
    "                    boxes = result.boxes\n",
    "                    if len(boxes) > 0:\n",
    "                        confidences = boxes.conf\n",
    "                        best_idx = np.argmax(confidences)\n",
    "                        cls_id = int(boxes.cls[best_idx].item())\n",
    "                        predicted_confidence = confidences[best_idx].item()\n",
    "                        predicted_label = self.class_names[cls_id]\n",
    "\n",
    "            if predicted_label and predicted_confidence >= confidence_threshold and time.time() - last_prediction_time > debounce_time:\n",
    "                last_prediction_time = time.time()\n",
    "                last_prediction = predicted_label\n",
    "                if len(gesture_history) >= 5:\n",
    "                    gesture_history.pop(0)\n",
    "                gesture_history.append(predicted_label)\n",
    "                if len(current_word) == 0 or (current_word and current_word[-1] != predicted_label):\n",
    "                    current_word.append(predicted_label)\n",
    "            \n",
    "            # Overlay for ROI with stable display\n",
    "            overlay = frame.copy()\n",
    "            cv2.rectangle(overlay, (roi_start_x, roi_start_y), (roi_end_x, roi_end_y), (255, 200, 150), -1)\n",
    "            alpha = 0.4\n",
    "            cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "            cv2.rectangle(frame, (roi_start_x, roi_start_y), (roi_end_x, roi_end_y), (0, 0, 255), 2)\n",
    "\n",
    "            # Key events for sentence formation\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord(' '):\n",
    "                if current_word:\n",
    "                    sentence += ''.join(current_word) + \" \"\n",
    "                    current_word = []\n",
    "            elif key == 13:\n",
    "                if current_word:\n",
    "                    sentence += ''.join(current_word) + \" \"\n",
    "                    current_word = []\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "            \n",
    "            # FPS Calculation\n",
    "            curr_time = time.time()\n",
    "            fps = 1 / (curr_time - prev_time) if curr_time - prev_time > 0 else 0\n",
    "            prev_time = curr_time\n",
    "            \n",
    "            # Display info\n",
    "            cv2.rectangle(frame, (20, screen_height - 120), (screen_width - 20, screen_height - 20), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, f'Current Word: {\"\".join(current_word)}', (30, screen_height - 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f'Sentence: {sentence.strip()}', (30, screen_height - 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f'FPS: {int(fps)}', (roi_start_x + 10, roi_end_y + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            \n",
    "            # Display datetime in non-overlapping area\n",
    "            datetime_text = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            cv2.putText(frame, datetime_text, (30, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "            \n",
    "            cv2.imshow(\"Hand Gesture Recognition\", frame)\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "model_path = \"runs/detect/hand_gesture_model/weights/best.pt\"\n",
    "hgr = HandGestureRecognition(model_path)\n",
    "hgr.real_time_detection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating pseudo-labels using teacher model...\n",
      "Pseudo-labels generated and saved.\n",
      "\n",
      "Starting training for student model using distilled labels...\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "teacher_model_path = \"runs/detect/hand_gesture_model/weights/best.pt\"\n",
    "student_model_path = \"yolov8n.pt\"  # Smaller model\n",
    "images_dir = \"D:\\objdetection_imp_model\\hand_gestures_dataset\\train\\images\"  # Replace with actual path\n",
    "distilled_labels_dir = \"distilled_labels/\"\n",
    "data_yaml = \"hand_gestures.yaml\"  # Replace with your dataset YAML\n",
    "\n",
    "# Load models\n",
    "teacher_model = YOLO(teacher_model_path)\n",
    "student_model = YOLO(student_model_path)\n",
    "\n",
    "# Step 1: Generate pseudo-labels using teacher model\n",
    "os.makedirs(distilled_labels_dir, exist_ok=True)\n",
    "\n",
    "print(\"\\nGenerating pseudo-labels using teacher model...\")\n",
    "for img_path in Path(images_dir).glob(\"*.jpg\"):\n",
    "    results = teacher_model(img_path)\n",
    "    results.save_txt(save_dir=distilled_labels_dir)\n",
    "\n",
    "print(\"Pseudo-labels generated and saved.\")\n",
    "\n",
    "# Step 2: Optional - Replace original labels with distilled labels\n",
    "# (This step depends on your dataset structure)\n",
    "# shutil.copytree(distilled_labels_dir, 'path/to/your/train/labels')\n",
    "\n",
    "# Step 3: Train the student model using YOLO CLI (Run this manually or via subprocess)\n",
    "print(\"\\nStarting training for student model using distilled labels...\")\n",
    "\n",
    "os.system(f\"yolo detect train model={student_model_path} data={data_yaml} epochs=50 imgsz=640 name=distilled_student\")\n",
    "\n",
    "print(\"\\nStudent model training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
